{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cc522a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tutorial - Integrating RAPIDS with BERTopic for Topic Modeling \n",
    "(last updated 08-19-2022)\n",
    "\n",
    "In this tutorial, we will use GPU to gauge the perfromance speedup that can be achieved by integrating RAPIDS with BERTopic. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/803/0*jRTe8f9xmQN3SCCX\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf91e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installing RAPIDS\n",
    "\n",
    "- First, you'll need to ensure GPUs are enabled for the notebook\n",
    "- Navigate to [RAPIDS](https://rapids.ai/start.html) to install latest CUDA driver and RAPIDS container\n",
    "- For this tutorial, I installed RAPIDS docker image for Python 3.9 and Ubuntu 20.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee579e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installing BERTopic and other necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10db8fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Installing BERTopic\n",
    "\n",
    "We need to ensure that HDBSCAN package version is compatible with the environment to seamlessly install BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329303f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08a5acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (1837.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp39-cp39-linux_x86_64.whl (23.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Downloading a speicific version of torch which is compatible with BERTopic\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbc6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/rapids\n",
      "\n",
      "  added / updated specs:\n",
      "    - hdbscan\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    certifi-2022.6.15          |   py39hf3d152e_0         155 KB  conda-forge\n",
      "    hdbscan-0.8.28             |   py39hce5d2b2_1         706 KB  conda-forge\n",
      "    openssl-1.1.1q             |       h166bdaf_0         2.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  hdbscan            conda-forge/linux-64::hdbscan-0.8.28-py39hce5d2b2_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  certifi                        2022.5.18.1-py39hf3d152e_0 --> 2022.6.15-py39hf3d152e_0\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "hdbscan-0.8.28       | 706 KB    | ##################################### | 100% \n",
      "certifi-2022.6.15    | 155 KB    | ##################################### | 100% \n",
      "openssl-1.1.1q       | 2.1 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Can be avoided if you want to pip install\n",
    "!conda install -c conda-forge hdbscan -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785bfd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::brotlipy==0.7.0=py39h3811e60_1003\n",
      "  - conda-forge/linux-64::certifi==2022.5.18.1=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::cffi==1.15.0=py39h4bc2ebd_0\n",
      "  - conda-forge/noarch::charset-normalizer==2.0.12=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::colorama==0.4.4=pyh9f0ad1d_0\n",
      "  - conda-forge/linux-64::conda==4.12.0=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::conda-package-handling==1.8.0=py39hb9d737c_0\n",
      "  - conda-forge/linux-64::cryptography==36.0.2=py39hd97740a_0\n",
      "  - conda-forge/noarch::idna==3.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pip==22.0.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pycosat==0.6.3=py39h3811e60_1009\n",
      "  - conda-forge/noarch::pycparser==2.21=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyopenssl==22.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pysocks==1.7.1=py39hf3d152e_4\n",
      "  - conda-forge/linux-64::python==3.9.10=h85951f9_2_cpython\n",
      "  - conda-forge/linux-64::python_abi==3.9=2_cp39\n",
      "  - conda-forge/linux-64::readline==8.1=h46c0cb4_0\n",
      "  - conda-forge/noarch::requests==2.27.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::ruamel_yaml==0.15.80=py39h3811e60_1006\n",
      "  - conda-forge/linux-64::setuptools==60.10.0=py39hf3d152e_0\n",
      "  - conda-forge/noarch::six==1.16.0=pyh6c4a22f_0\n",
      "  - conda-forge/linux-64::sqlite==3.37.1=h4ff8645_0\n",
      "  - conda-forge/noarch::tqdm==4.63.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::urllib3==1.26.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::wheel==0.37.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::mamba==0.15.3=py39h951de11_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
      "    libarchive-3.5.2           |       hb890918_3         1.6 MB  conda-forge\n",
      "    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  conda-forge\n",
      "    libgomp-12.1.0             |      h8d9b700_16         459 KB  conda-forge\n",
      "    libssh2-1.10.0             |       haa6b8db_3         234 KB  conda-forge\n",
      "    libxml2-2.9.14             |       h22db469_4         771 KB  conda-forge\n",
      "    libzlib-1.2.12             |       h166bdaf_2          63 KB  conda-forge\n",
      "    ncurses-6.3                |       h27087fc_1        1002 KB  conda-forge\n",
      "    tzdata-2022b               |       h191b570_0         118 KB  conda-forge\n",
      "    xz-5.2.6                   |       h166bdaf_0         409 KB  conda-forge\n",
      "    zlib-1.2.12                |       h166bdaf_2          91 KB  conda-forge\n",
      "    zstd-1.5.2                 |       h8a70e8d_4         448 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.0 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  _openmp_mutex                                   4.5-1_gnu --> 4.5-2_gnu\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  libarchive                               3.5.2-hb890918_2 --> 3.5.2-hb890918_3\n",
      "  libgcc-ng                              11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libgomp                                11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libssh2                                 1.10.0-ha56f1ee_2 --> 1.10.0-haa6b8db_3\n",
      "  libxml2                                 2.9.14-h22db469_0 --> 2.9.14-h22db469_4\n",
      "  libzlib                              1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  ncurses                                    6.3-h9c3ff4c_0 --> 6.3-h27087fc_1\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "  tzdata                                   2022a-h191b570_0 --> 2022b-h191b570_0\n",
      "  xz                                       5.2.5-h516909a_1 --> 5.2.6-h166bdaf_0\n",
      "  zlib                                 1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  zstd                                     1.5.2-h8a70e8d_1 --> 1.5.2-h8a70e8d_4\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \n",
      "libarchive-3.5.2     | 1.6 MB    | ##################################### | 100% \n",
      "ncurses-6.3          | 1002 KB   | ##################################### | 100% \n",
      "tzdata-2022b         | 118 KB    | ##################################### | 100% \n",
      "libxml2-2.9.14       | 771 KB    | ##################################### | 100% \n",
      "libgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \n",
      "zstd-1.5.2           | 448 KB    | ##################################### | 100% \n",
      "libssh2-1.10.0       | 234 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 91 KB     | ##################################### | 100% \n",
      "libzlib-1.2.12       | 63 KB     | ##################################### | 100% \n",
      "xz-5.2.6             | 409 KB    | ##################################### | 100% \n",
      "libgomp-12.1.0       | 459 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f8ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/rapids/lib/python3.9/site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m145.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hdbscan in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.8.28)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.37.1)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (0.29.30)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (0.24.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Can be avoided if HDBSCAN is installed from Conda\n",
    "!pip install --upgrade pip hdbscan wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0670a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.11.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.24.2)\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m178.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hdbscan>=0.8.28 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.8.28)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: umap-learn>=0.5.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.5.3)\n",
      "Collecting pyyaml<6.0\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 kB\u001b[0m \u001b[31m219.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.30)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m226.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.12.1+cu113)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.1+cu113)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m258.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m258.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pynndescent>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.7)\n",
      "Requirement already satisfied: numba>=0.49 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (60.10.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m253.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.1/765.1 kB\u001b[0m \u001b[31m229.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125940 sha256=39a50fd268f9ad01c09c0b00c260395c4b668a2e37b14fcda1786cd6ed5583ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, tenacity, regex, pyyaml, filelock, plotly, nltk, huggingface-hub, transformers, sentence-transformers, bertopic\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.6.0 requires cupy-cuda115, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bertopic-0.11.0 filelock-3.8.0 huggingface-hub-0.8.1 nltk-3.7 plotly-5.10.0 pyyaml-5.4.1 regex-2022.7.25 sentence-transformers-2.2.2 sentencepiece-0.1.97 tenacity-8.0.1 tokenizers-0.12.1 transformers-4.21.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bertopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a50ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access folders\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0850490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access Amazon S3 buckets\n",
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffaec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Installing package needed for downloading Wikidata Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08a917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beam\n",
      "  Downloading apache_beam-2.40.0-cp39-cp39-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m200.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0\n",
      "  Downloading orjson-3.7.11-cp39-cp39-manylinux_2_28_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (7.0.0)\n",
      "Collecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.1.0)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m213.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2022.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (4.2.0)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.8.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.27.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Collecting grpcio<2,>=1.33.1\n",
      "  Downloading grpcio-1.47.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m234.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (3.20.1)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from grpcio<2,>=1.33.1->apache_beam) (1.16.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from httplib2<0.21.0,>=0.8->apache_beam) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18848 sha256=fe70632bbfa39dacfd2d0d2e8d28c233001fd0fdf27400323d727c4efdc2128e\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=934c3bb485ae5ae71a73e51ed448a67bbfa14beda01f6a81a11cd05c84c1222b\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=144cf9ca0050bbf41b5ef3770863a9c60f604dba01c78858009c9903abc20265\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built crcmod dill docopt\n",
      "Installing collected packages: docopt, crcmod, pymongo, pydot, proto-plus, orjson, mwparserfromhell, httplib2, grpcio, dill, hdfs, apache_beam\n",
      "Successfully installed apache_beam-2.40.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 grpcio-1.47.0 hdfs-2.7.0 httplib2-0.20.4 mwparserfromhell-0.6.4 orjson-3.7.11 proto-plus-1.22.0 pydot-1.4.2 pymongo-3.12.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1639a273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.3.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.4.0 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86671e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Restart the Notebook\n",
    "After installing BERTopic, we should restart the notebook to correctly use the updated packages.\n",
    "\n",
    "Go to Menu -> Runtime → Restart Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c1719",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392e0743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary packages\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import rmm\n",
    "import os\n",
    "# import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('max_colwidth', -1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "rmm.reinitialize(pool_allocator=True,initial_pool_size=5e+9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89c44ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if GPU is working\n",
    "import torch \n",
    "torch.tensor([1.0,2.0]).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf37cf1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We are using 4 publicly available datasets for demonstration\n",
    "- Newsgroups dataset containing roughly 18000 newsgroups posts.\n",
    "- Contract Understanding Atticus Dataset (CUAD) dataset corpus consisting of more than 13,000 labels in 510 commercial legal contracts. \n",
    "- AN4 dataset consisting of spectrogram information that is transformed using Soumith's audio library.\n",
    "- Wikidata corpus built from the Wikipedia dump (https://dumps.wikimedia.org/) contains the contents of multiple Wikipedia articles.\n",
    "\n",
    "To add additional datasets,\n",
    "- Put datasets in data folder. Code is generalized to handle additional datasets that are put in data folder.\n",
    "- But import datasets need to manually added in a list 'bucket_list' that keeps track of all the datasets to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5dd35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing custom datasets for the lab\n",
    "#All the datasets names are stored in a list 'bucket_list' for ease of tracking \n",
    "bucket_list = []\n",
    "\n",
    "#Importing news dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "bucket_list.append('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296531ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code will pickup files from data folder and add it to bucket_list\n",
    "data_folder = \"./data/\"\n",
    "list_files = os.listdir(data_folder)\n",
    "for i in list_files:\n",
    "    if i.find(\".md\")==-1:\n",
    "        bucket_list.append(data_folder+i)\n",
    "\n",
    "bucket_list\n",
    "\n",
    "#Following lines of code help with picking up data files from S3 bucket\n",
    "#Loading data from S3 bucket\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_bucket_name = 'nvidia-rapids'\n",
    "# s3 = boto3.resource('s3')\n",
    "# my_bucket=s3.Bucket(s3_bucket_name)\n",
    "\n",
    "# for file in my_bucket.objects.filter():\n",
    "#     file_name=file.key\n",
    "#     if file_name!=\"data/\" and file_name.find(\"data/\")!=-1:\n",
    "#         bucket_list.append(file.key)\n",
    "# bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b0c2587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 35.9kB [00:00, 14.4MB/s]                   \n",
      "Downloading metadata: 30.4kB [00:00, 19.1MB/s]                   \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not enough disk space. Needed: 38.07 GiB (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Importing Wikidata corpus\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m data_wiki \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20220301.en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m bucket_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_wiki\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m bucket_list\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/load.py:1746\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1746\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_verifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1756\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1757\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:650\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local:  \u001b[38;5;66;03m# if cache dir is local, check for available space\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_sufficient_disk_space(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msize_in_bytes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m, directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir_root):\n\u001b[0;32m--> 650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough disk space. Needed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msize_in_bytes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (download: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, post-processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mpost_processing_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m         )\n\u001b[1;32m    654\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mincomplete_dir\u001b[39m(dirname):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;124;03m\"\"\"Create temporary dir for dirname and rename on exit.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough disk space. Needed: 38.07 GiB (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size)"
     ]
    }
   ],
   "source": [
    "#Importing Wikidata corpus\n",
    "from datasets import load_dataset\n",
    "data_wiki = pd.DataFrame(load_dataset(\"wikipedia\", \"20220301.en\"))\n",
    "bucket_list.append('data_wiki')\n",
    "\n",
    "bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cef8719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to read the data files\n",
    "data_wiki_rows = 500000  #As Wikipedia data is huge, to ensure a lower runtime, we can update this variable to pickup these many number of rows \n",
    "for i in range(len(bucket_list)):\n",
    "    if bucket_list[i].find(\".json\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_json(bucket_list[i], orient='columns')\n",
    "    elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "        # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "    elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_pickle(bucket_list[i])\n",
    "    elif bucket_list[i]=='docs':\n",
    "        globals()[\"data_\" + str(i)] = docs\n",
    "    elif bucket_list[i]=='data_wiki':\n",
    "        globals()[\"data_\" + str(i)] = data_wiki.head(data_wiki_rows)\n",
    "    else:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i])\n",
    "\n",
    "#Following lines of code help with reading data files from S3 bucket\n",
    "# for i in range(len(bucket_list)):\n",
    "#     if bucket_list[i].find(\".json\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_json('s3://'+s3_bucket_name+'/'+bucket_list[i], orient='columns')\n",
    "#     elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "#         # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "#     elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_pickle('s3://'+s3_bucket_name+'/'+bucket_list[i])\n",
    "#     elif bucket_list[i]=='docs':\n",
    "#         globals()[\"data_\" + str(i)] = docs\n",
    "#     elif bucket_list[i]=='data_wiki':\n",
    "#         globals()[\"data_\" + str(i)] = data_wiki\n",
    "#     else:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31e17b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Sample Text cleaning before fitting into model is applied here. \n",
    "- Most NLP corpus needs customized cleaning based on datasets.\n",
    "- But for the purpose of demonstration, we are only keeps words as topics and removing numbers and other special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d85a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize dataset 1(in s): 4.54\n",
      "Time to vectorize dataset 2(in s): 4.97\n",
      "Time to vectorize dataset 3(in s): 0.54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to vectorize dataset 3(in s): \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(end_time, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m     29\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m data_3 \u001b[38;5;241m=\u001b[39m \u001b[43mdata_3\u001b[49m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(pat1,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(pat2,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     31\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to vectorize dataset 4(in s): \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(end_time, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_3' is not defined"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing step\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "pat1=re.compile(r\"[^a-zA-Z ]+\")\n",
    "pat2=re.compile(r'\\b(?:{})\\b'.format('|'.join(stop)))\n",
    "\n",
    "#Data cleaning for various datasets\n",
    "start_time = time.time()\n",
    "data_0 = pd.DataFrame(data_0)\n",
    "data_0.columns = ['train']\n",
    "data_0 = data_0.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 1(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_1 = data_1.data.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 2(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_2 = data_2[0].astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 3(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_3 = data_3.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 4(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "# def random_function(x):\n",
    "#     return ((str(x).replace(r\"[^a-zA-Z ]+\", \" \")).strip())\n",
    "# data_0 = np.vectorize(random_function)(data_0.data) \n",
    "# data_1 = np.vectorize(random_function)(data_1[0].head(20000))\n",
    "# data_3=data_3.head(10000)\n",
    "# data_0 = data_0.data.apply(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "# data_3=data_3['train'].astype(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f71854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 info: \n",
      "Size: 30896294\n",
      "Memory: 301536\n",
      "\n",
      "---------\n",
      "Dataset1 info: \n",
      "Size: 34454328\n",
      "Memory: 8160\n",
      "\n",
      "---------\n",
      "Dataset2 info: \n",
      "Size: 15418194\n",
      "Memory: 3634288\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Storing pre-procssed data into a folder to prevent the need of data pre-processing in future iterations\n",
    "processed_data_folder = \"./processed_data/\"\n",
    "for i in range(len(bucket_list)):\n",
    "    print(\"Dataset\"+str(i)+\" info: \")\n",
    "    globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].dropna()\n",
    "    # print(globals()[\"data_\" + str(i)].info())\n",
    "    print(\"Size: \"+str(sys.getsizeof(globals()[\"data_\" + str(i)])))\n",
    "    print(\"Memory: \"+str((globals()[\"data_\" + str(i)]).memory_usage(index=True)))\n",
    "    print()\n",
    "    print(\"---------\")\n",
    "    globals()[\"data_\" + str(i)].to_csv(processed_data_folder+\"data_\"+str(i)+\".csv\", header=[\"data\"],index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ad5eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Reloading\n",
    "\n",
    "This step is helpful when data loading and preprocessing has happened once, and we do not want to relaod the data again.\n",
    "- There are some data cleaning steps like removal of text which only consist of numbers or duplicate text, but it should be customized based on dataset\n",
    "- You can avoid this step during the first run as all the dataset variables point to right dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30ec0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from the processed folder so that we can skip data load and preprocessing in future runs\n",
    "read_from_processed = False   #Change it to True if you want to load data from processed folder\n",
    "bucket_list = ['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz', 'data_wiki']\n",
    "if read_from_processed==True:\n",
    "    for i in range(len(bucket_list)):\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(\"processed_data/data_\"+str(i)+\".csv\", dtype=str,index_col=None)\n",
    "        # We can do some additional cleaning like sentences that only have numbers\n",
    "        globals()[\"data_\" + str(i)].data = globals()[\"data_\" + str(i)].apply(lambda r: r['data'] if type(r['data'])!=float else np.nan, axis=1)\n",
    "        globals()[\"data_\" + str(i)].dropna(inplace=True)\n",
    "        globals()[\"data_\" + str(i)].drop_duplicates(inplace=True)\n",
    "        globals()[\"data_\" + str(i)].reset_index(drop=True, inplace=True)\n",
    "        globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cf9ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Topic Modeling using BERTopic [without RAPIDS integration]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27bb60d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training the BERTopic model\n",
    "\n",
    "For instantiating BERTopic, you need to run the following commands\n",
    "- topic_model = BERTopic(verbose=True)\n",
    "- topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "We can customize BERTopic by using the different parameters mentioned in [BERTopic](https://maartengr.github.io/BERTopic/faq.html). You can add parameters like hdbscan_model=HDBSCAN(), umap_model=UMAP(), language=\"english\" or calculate_probabilities=True in BERTopic() to customize the model.\n",
    "\n",
    "We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c941e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  1 05:09:38 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 589/589 [00:09<00:00, 59.31it/s] \n",
      "2022-09-01 05:09:49,882 - BERTopic - Transformed documents to Embeddings\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2022-09-01 05:10:07,484 - BERTopic - Reduced dimensionality\n",
      "2022-09-01 05:10:08,615 - BERTopic - Clustered reduced embeddings\n",
      "2022-09-01 05:10:17,747 - BERTopic - Reduced number of topics from 603 to 127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 39.69\n",
      "Thu Sep  1 05:10:44 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 11.06it/s]\n",
      "2022-09-01 05:10:48,076 - BERTopic - Transformed documents to Embeddings\n",
      "2022-09-01 05:10:49,928 - BERTopic - Reduced dimensionality\n",
      "2022-09-01 05:10:49,949 - BERTopic - Clustered reduced embeddings\n",
      "2022-09-01 05:10:56,714 - BERTopic - Reduced number of topics from 25 to 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset2(in s): 12.02\n",
      "Thu Sep  1 05:11:03 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7099/7099 [00:28<00:00, 248.77it/s]\n",
      "2022-09-01 05:11:33,562 - BERTopic - Transformed documents to Embeddings\n",
      "2022-09-01 05:16:10,446 - BERTopic - Reduced dimensionality\n",
      "2022-09-01 05:16:24,543 - BERTopic - Clustered reduced embeddings\n",
      "2022-09-01 05:16:52,058 - BERTopic - Reduced number of topics from 6098 to 138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset3(in s): 348.39\n"
     ]
    }
   ],
   "source": [
    "visualize = True  #Parameter to decide if you want the visual results of BERTopic or not\n",
    "for i in range(len(bucket_list)):\n",
    "    if i>2:   #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"topic_model_gpu_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\", hdbscan_model=HDBSCAN(), umap_model=UMAP())\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_gpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_gpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_gpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    if visualize == True:\n",
    "        viz_topics = globals()[\"topic_model_gpu_\" + str(i)].visualize_topics()\n",
    "        # print(viz_topics.show())\n",
    "        viz_topics.write_html(\"results/viz_topics_\"+str(i)+\".html\")\n",
    "        viz_barchart = globals()[\"topic_model_gpu_\" + str(i)].visualize_barchart()\n",
    "        # print(viz_barchart.show())\n",
    "        viz_barchart.write_html(\"results/viz_barchart_\"+str(i)+\".html\")\n",
    "        viz_docs = globals()[\"topic_model_gpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "        viz_docs.write_html(\"results/viz_docs_\"+str(i)+\".html\")\n",
    "# globals()[\"topic_model_gpu_\" + str(i)] = globals()[\"topic_model_gpu_\" + str(i)].fit(globals()[\"data_\" + str(i)])\n",
    "# globals()[\"topics_\" + str(i)] = globals()[\"topic_model_gpu_\" + str(i)]._map_predictions(globals()[\"topic_model_gpu_\" + str(i)].hdbscan_model.labels_) \n",
    "# globals()[\"probs_\" + str(i)] = globals()[\"topic_model_gpu_\" + str(i)].hdbscan_model.probabilities_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722eed73",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting topics\n",
    "\n",
    "- After fitting the model, we usually look at the most frequent topics first as they best represent the collection of documents.\n",
    "- -1 should be ignored as it refers to outliers. \n",
    "\n",
    "**NOTE**: Due to stochastic nature of UMAP stage of BERTopic, the topics might differ across runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2215bfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7075</td>\n",
       "      <td>-1_edu_com_the_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>0_baseball_game_year_team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>1_key_encryption_clipper_chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>2_gun_guns_firearms_militia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>3_car_cars_mustang_engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>291</td>\n",
       "      <td>10</td>\n",
       "      <td>291_kent_mcs_mhamilto_nimitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>292</td>\n",
       "      <td>10</td>\n",
       "      <td>292_motivated_religously_religiously_frank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>293</td>\n",
       "      <td>10</td>\n",
       "      <td>293_liar_rh_lunatic_gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>294</td>\n",
       "      <td>10</td>\n",
       "      <td>294_clinton_reno_federal_batf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>295</td>\n",
       "      <td>10</td>\n",
       "      <td>295_viewer_tmc_dominik_dundee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                        Name\n",
       "0   -1      7075   -1_edu_com_the_subject                    \n",
       "1    0      857    0_baseball_game_year_team                 \n",
       "2    1      780    1_key_encryption_clipper_chip             \n",
       "3    2      306    2_gun_guns_firearms_militia               \n",
       "4    3      279    3_car_cars_mustang_engine                 \n",
       "..  ..      ...                          ...                 \n",
       "292  291    10     291_kent_mcs_mhamilto_nimitz              \n",
       "293  292    10     292_motivated_religously_religiously_frank\n",
       "294  293    10     293_liar_rh_lunatic_gathered              \n",
       "295  294    10     294_clinton_reno_federal_batf             \n",
       "296  295    10     295_viewer_tmc_dominik_dundee             \n",
       "\n",
       "[297 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_gpu_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a0bfd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseball', 0.010475889952278009),\n",
       " ('game', 0.00888648349492628),\n",
       " ('year', 0.00861105105443462),\n",
       " ('team', 0.008469061639788836),\n",
       " ('players', 0.007548359382496322),\n",
       " ('hit', 0.007530132235814117),\n",
       " ('braves', 0.007529716506300251),\n",
       " ('games', 0.007309955232745601),\n",
       " ('runs', 0.0068805481485816585),\n",
       " ('pitching', 0.006444728759187212)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_gpu_0.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ace0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>159</td>\n",
       "      <td>-1_agreement_party_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0_distributor_agreement_party_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1_agreement_party_company_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2_sponsorship_sponsor_agreement_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>3_maintenance_agreement_shall_vendor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4_endorsement_contract_agreement_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5_co_verticalnet_branding_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>6_hosting_customer_software_agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>7_product_shall_agreement_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_msl_ibm_customer_outsourcing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9_spinco_group_intellectual_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>10_franchise_franchisee_us_pretzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_transportation_shipper_transporter_shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12_reseller_agreement_ehave_touchstar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13_shall_party_astellas_roundup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name\n",
       "0  -1      159    -1_agreement_party_shall_contract          \n",
       "1   0      59     0_distributor_agreement_party_ex           \n",
       "2   1      58     1_agreement_party_company_contract         \n",
       "3   2      32     2_sponsorship_sponsor_agreement_contract   \n",
       "4   3      31     3_maintenance_agreement_shall_vendor       \n",
       "5   4      22     4_endorsement_contract_agreement_ex        \n",
       "6   5      21     5_co_verticalnet_branding_party            \n",
       "7   6      20     6_hosting_customer_software_agreement      \n",
       "8   7      19     7_product_shall_agreement_party            \n",
       "9   8      17     8_msl_ibm_customer_outsourcing             \n",
       "10  9      16     9_spinco_group_intellectual_property       \n",
       "11  10     16     10_franchise_franchisee_us_pretzel         \n",
       "12  11     15     11_transportation_shipper_transporter_shall\n",
       "13  12     13     12_reseller_agreement_ehave_touchstar      \n",
       "14  13     12     13_shall_party_astellas_roundup            "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_gpu_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5516159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distributor', 0.037079294786719036),\n",
       " ('agreement', 0.03270249443242861),\n",
       " ('party', 0.030775109377613134),\n",
       " ('ex', 0.026850783858802674),\n",
       " ('contract', 0.026406133174178816),\n",
       " ('shall', 0.026399277154165558),\n",
       " ('license', 0.022018705365346277),\n",
       " ('related', 0.019295638152168407),\n",
       " ('licensee', 0.01887526968572377),\n",
       " ('parts', 0.01856044081063633)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_gpu_1.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a5252a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Topic Modeling using RAPIDS[cuML] integrated BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159789a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training the RAPIDS integrated BERTopic model\n",
    "\n",
    "For instantiating RAPIDS integrated BERTopic, you need to run the following commands\n",
    "- topic_model = BERTopic(verbose=True, hdbscan_model=HDBSCAN_gpu(), umap_model=UMAP_gpu())\n",
    "- topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "As we see above, RAPIDS is integrated with BERTopic in such a way that we can use RAPIDS cuML library with couple of changes in BERTopic() parameters.\n",
    "\n",
    "We can customize BERTopic by using the different parameters mentioned in [BERTopic](https://maartengr.github.io/BERTopic/faq.html#can-i-use-the-gpu-to-speed-up-the-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a40597ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from cuml.cluster import HDBSCAN as HDBSCAN_gpu\n",
    "from cuml.manifold import UMAP as UMAP_gpu\n",
    "from cuml.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "76ced310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 01:23:00 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8439418eeee6479b8f42f1e8c95f4035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:23:14,864 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 01:23:15,055 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 01:23:15,426 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 01:23:25,731 - BERTopic - Reduced number of topics from 131 to 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 25.07\n",
      "Mon Aug 15 01:23:52 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921a484d3db348e3b40f2b5171825675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:23:57,916 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 01:23:57,947 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 01:23:58,098 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 01:24:06,724 - BERTopic - Reduced number of topics from 12 to 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset2(in s): 14.05\n",
      "Mon Aug 15 01:24:14 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a6d2cac887464b9703cb0034cd769c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 01:24:50,108 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 01:25:09,170 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 01:25:16,469 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 01:25:18,804 - BERTopic - Reduced number of topics from 830 to 99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset3(in s): 64.33\n"
     ]
    }
   ],
   "source": [
    "visualize = True  #Set to True if you want to visualize the topics \n",
    "for i in range(len(bucket_list)):\n",
    "    if i>2:     #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"umap_model_\" + str(i)] = UMAP_gpu(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "    globals()[\"hdbscan_model_\" + str(i)] = HDBSCAN_gpu(min_samples=10, gen_min_span_tree=True)\n",
    "    # Pass the above models to be used in BERTopic\n",
    "    globals()[\"topic_model_cubert_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",umap_model=globals()[\"umap_model_\" + str(i)], hdbscan_model=globals()[\"hdbscan_model_\" + str(i)])\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cubert_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cubert_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cubert_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    if visualize == True:\n",
    "        viz_topics = globals()[\"topic_model_cubert_\" + str(i)].visualize_topics()\n",
    "        # print(viz_topics.show())\n",
    "        viz_topics.write_html(\"results/viz_topics_cu_\"+str(i)+\".html\")\n",
    "        viz_barchart = globals()[\"topic_model_cubert_\" + str(i)].visualize_barchart()\n",
    "        # print(viz_barchart.show())\n",
    "        viz_barchart.write_html(\"results/viz_barchart_cu_\"+str(i)+\".html\")\n",
    "        viz_docs = globals()[\"topic_model_cubert_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "        viz_docs.write_html(\"results/viz_docs_cu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d1a75c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bfe98b19794e1b84e97d0a7c1b3ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:22:23,941 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-14 01:22:25,263 - BERTopic - Reduced dimensionality\n",
      "2022-08-14 01:22:25,617 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-14 01:22:34,626 - BERTopic - Reduced number of topics from 135 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 27.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78af5a21c12b4475b406ec92e0f56bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:22:38,841 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-14 01:22:38,867 - BERTopic - Reduced dimensionality\n",
      "2022-08-14 01:22:39,008 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-14 01:22:46,968 - BERTopic - Reduced number of topics from 16 to 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset2(in s): 12.34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2979aa0cde644ca896345a36c4faa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 01:23:25,953 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-14 01:23:45,093 - BERTopic - Reduced dimensionality\n",
      "2022-08-14 01:23:52,400 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-14 01:23:54,736 - BERTopic - Reduced number of topics from 830 to 105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset3(in s): 67.77\n"
     ]
    }
   ],
   "source": [
    "# Results of another run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "24d99544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 07:28:30,891 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-14 07:29:42,060 - BERTopic - Reduced dimensionality\n",
      "2022-08-14 07:29:58,296 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset4(in s): 1334.17\n"
     ]
    }
   ],
   "source": [
    "#Due to huge size of wikipedia data, it was run separately\n",
    "visualize = True\n",
    "for i in range(len(bucket_list)):\n",
    "    if i<=2:     #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"umap_model_\" + str(i)] = UMAP_gpu(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "    globals()[\"hdbscan_model_\" + str(i)] = HDBSCAN_gpu(min_samples=10, gen_min_span_tree=True)\n",
    "    # Pass the above models to be used in BERTopic\n",
    "    globals()[\"topic_model_cubert_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",umap_model=globals()[\"umap_model_\" + str(i)], hdbscan_model=globals()[\"hdbscan_model_\" + str(i)])\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cubert_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cubert_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cubert_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    if visualize == True:\n",
    "        viz_topics = globals()[\"topic_model_cubert_\" + str(i)].visualize_topics()\n",
    "        # print(viz_topics.show())\n",
    "        viz_topics.write_html(\"results/viz_topics_cu_\"+str(i)+\".html\")\n",
    "        viz_barchart = globals()[\"topic_model_cubert_\" + str(i)].visualize_barchart()\n",
    "        # print(viz_barchart.show())\n",
    "        viz_barchart.write_html(\"results/viz_barchart_cu_\"+str(i)+\".html\")\n",
    "        viz_docs = globals()[\"topic_model_cubert_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "        viz_docs.write_html(\"results/viz_docs_cu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83bf4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Extracting topics\n",
    "\n",
    "- After fitting the model, we usually look at the most frequent topics first as they best represent the collection of documents.\n",
    "- -1 should be ignored as it refers to outliers. \n",
    "\n",
    "**NOTE**: Due to stochastic nature of UMAP stage of BERTopic, the topics might differ across runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a76a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7967</td>\n",
       "      <td>-1_edu_the_com_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1855</td>\n",
       "      <td>0_game_team_hockey_games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1807</td>\n",
       "      <td>1_car_bike_dod_com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>818</td>\n",
       "      <td>2_medical_disease_cancer_msg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>692</td>\n",
       "      <td>3_space_nasa_launch_orbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>131_joystick_int_port_eyal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "      <td>132_synoptics_klee_widgets_widget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>133</td>\n",
       "      <td>5</td>\n",
       "      <td>133_baptists_trincoll_handheld_banging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>134</td>\n",
       "      <td>5</td>\n",
       "      <td>134_typing_rsi_keyboard_berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>135</td>\n",
       "      <td>5</td>\n",
       "      <td>135_hannover_sampling_heinboke_adc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                    Name\n",
       "0   -1      7967   -1_edu_the_com_subject                \n",
       "1    0      1855   0_game_team_hockey_games              \n",
       "2    1      1807   1_car_bike_dod_com                    \n",
       "3    2      818    2_medical_disease_cancer_msg          \n",
       "4    3      692    3_space_nasa_launch_orbit             \n",
       "..  ..      ...                          ...             \n",
       "132  131    5      131_joystick_int_port_eyal            \n",
       "133  132    5      132_synoptics_klee_widgets_widget     \n",
       "134  133    5      133_baptists_trincoll_handheld_banging\n",
       "135  134    5      134_typing_rsi_keyboard_berkeley      \n",
       "136  135    5      135_hannover_sampling_heinboke_adc    \n",
       "\n",
       "[137 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cubert_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c51725ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 0.014723341857636218),\n",
       " ('team', 0.014033122834136701),\n",
       " ('hockey', 0.010271081247856223),\n",
       " ('games', 0.009663337017870146),\n",
       " ('players', 0.009230325117226821),\n",
       " ('year', 0.009042694970761444),\n",
       " ('season', 0.008809628956061221),\n",
       " ('play', 0.007999885530012555),\n",
       " ('baseball', 0.00795392384225527),\n",
       " ('edu', 0.007310806833653132)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cubert_0.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7fc6a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>481</td>\n",
       "      <td>0_agreement_shall_party_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1_spinco_intellectual_group_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2_reseller_agreement_shall_party</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                  Name\n",
       "0  0      481    0_agreement_shall_party_contract    \n",
       "1  1      16     1_spinco_intellectual_group_property\n",
       "2  2      13     2_reseller_agreement_shall_party    "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cubert_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5368872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('agreement', 0.061061634547482896),\n",
       " ('shall', 0.051238800336885),\n",
       " ('party', 0.04987079300098038),\n",
       " ('contract', 0.04087231522561693),\n",
       " ('ex', 0.030811017910416637),\n",
       " ('related', 0.027721035114121623),\n",
       " ('parts', 0.025278805981154232),\n",
       " ('details', 0.02523013458650461),\n",
       " ('question', 0.024984074548969094),\n",
       " ('reviewed', 0.024980114554129226)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cubert_1.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3182cdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b68220",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6de42d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall time is calculated for only the topic modeling piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame()\n",
    "\n",
    "results['dataset'] = ['news_dataset', 'CUAD_dataset','AN4_dataset']\n",
    "results['BERTopic_time (in s)'] = [np.round(end_time_gpu_0,decimals=2),np.round(end_time_gpu_1,decimals=2),np.round(end_time_gpu_2,decimals=2)]\n",
    "results['BERTopic_time_with_RAPIDS (in s)'] = [np.round(end_time_cubert_0,decimals=2), np.round(end_time_cubert_1,decimals=2),np.round(end_time_cubert_2,decimals=2)]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "#         dataset       |    BERTopic_time (in s)  |     BERTopic_time [with RAPIDS] (in s)\n",
    "#    --------------     |   --------------------   |     --------------------------------\n",
    "# 0 |   news_dataset    |        37.59             |              27.11\n",
    "# 1 |   CUAD_dataset    |        15.86             |              12.34\n",
    "# 2 |   AN4_dataset     |        1823.34           |              67.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c3c5c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results/overall_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b8a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Deep dive into datasets - Performances of BERTopic with and without RAPIDS integration are gauged in various stages of Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a947e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERTopic Stages [without RAPIDS integration]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759ad5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Topic Modeling of AN4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19113cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bert_model = BERTopic(verbose=True, nr_topics=\"auto\")\n",
    "data=data_2\n",
    "docs = pd.DataFrame({\"Document\": data, \"ID\":range(len(data)), \"Topic\":None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6683ba7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfcbca272294020bca794c7bdf1d9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 15.7 s, total: 3min 2s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Extract embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings=model.encode(data, batch_size=64, show_progress_bar=True\n",
    "                        # ,#device='cpu'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a253928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 21:47:00,201 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 56min 15s, sys: 16.7 s, total: 2h 56min 32s\n",
      "Wall time: 27min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Dimensionality Reduction using UMAP\n",
    "umap_embeddings = topic_bert_model._reduce_dimensionality(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98dec082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 21:49:37,330 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 2.16 s, total: 55 s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Cluster UMAP embeddings with HDBSCAN\n",
    "docs, probs = topic_bert_model._cluster_embeddings(umap_embeddings, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ff11a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 417 ms, sys: 32 ms, total: 449 ms\n",
      "Wall time: 448 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Sort and map Topic IDs by frequency\n",
    "if not topic_bert_model.nr_topics:\n",
    "    docs = topic_bert_model._sort_mappings_by_frequency(docs)\n",
    "\n",
    "#Extract topics by calculating c-TF-IDF\n",
    "topic_bert_model._extract_topics(docs) #For topic extraction and representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133a252",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Topic Modeling of Wikidata dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ca833a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_bert_model = BERTopic(verbose=True, nr_topics=\"auto\")\n",
    "data=data_3\n",
    "docs = pd.DataFrame({\"Document\": data, \"ID\":range(len(data)), \"Topic\":None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e324a602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b852b235644b94888f45f550d8ed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 53min 29s, sys: 1min 7s, total: 1h 54min 37s\n",
      "Wall time: 11min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Extract embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings=model.encode(data, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "caffe21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-14 23:14:45,145 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 57min 5s, sys: 1min 18s, total: 2h 58min 24s\n",
      "Wall time: 13min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Dimensionality Reduction using UMAP\n",
    "umap_embeddings = topic_bert_model._reduce_dimensionality(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6505f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 00:18:02,205 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.1 s, sys: 5.36 s, total: 59.5 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Cluster UMAP embeddings with HDBSCAN\n",
    "docs, probs = topic_bert_model._cluster_embeddings(umap_embeddings, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "476922a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 16s, sys: 8.47 s, total: 8min 24s\n",
      "Wall time: 8min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Sort and map Topic IDs by frequency\n",
    "if not topic_bert_model.nr_topics:\n",
    "    docs = topic_bert_model._sort_mappings_by_frequency(docs)\n",
    "\n",
    "#Extract topics by calculating c-TF-IDF\n",
    "topic_bert_model._extract_topics(docs) #For topic extraction and representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb69d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## cuBERTopic Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2606f8",
   "metadata": {},
   "source": [
    "cuBERT is a standalone package that is built to optimize UMAP phase of BERTopic \n",
    "- To gauge the performance of various stages of cuBERTopic, we need to fork the git repository from [RAPIDS git repo](https://github.com/rapidsai/rapids-examples/tree/main/cuBERT_topic_modeling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09f3b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2f7ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Topic Modeling of AN4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ab34cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the below path to reflect location where you cloned the RAPIDS github repo\n",
    "cu_bert_path = '../rapids-examples/cuBERT_topic_modelling/'\n",
    "os.chdir(cu_bert_path)\n",
    "from cuBERTopic import gpu_BERTopic\n",
    "from embedding_extraction import create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "579c7682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', '0'], dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_cubert_model = gpu_BERTopic()\n",
    "data = \"data_2\"\n",
    "test_dataset = cudf.read_csv('../../NVIDIA-Devrel/processed_data/'+ data +'.csv')\n",
    "test_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4cd85844",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_dataset['0']\n",
    "docs = cudf.DataFrame({\"Document\": data, \"ID\":cp.arange(len(data)), \"Topic\":None})\n",
    "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "78d45358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 39.1 ms, total: 2min 11s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Extract embeddings\n",
    "embeddings = create_embeddings(docs.Document, embedding_model, vocab_file='../cuBERT_topic_modelling/vocab/voc_hash.txt')\n",
    "# model.encode(data, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9297264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.04 s, sys: 112 ms, total: 7.15 s\n",
      "Wall time: 7.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Dimensionality Reduction using UMAP\n",
    "umap_embeddings = topic_cubert_model.reduce_dimensionality(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "741f758b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.6 s, sys: 128 ms, total: 6.73 s\n",
      "Wall time: 6.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Cluster UMAP embeddings with HDBSCAN\n",
    "docs_cubert_model, probs_cubert_model = topic_cubert_model.cluster_embeddings(umap_embeddings, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "004c5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # c-TF-IDF \n",
    "# tf_idf, vectorizer, topic_labels = topic_cubert_model.create_topics(docs_cubert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cc542f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Topic representation\n",
    "# top_n_words, name_repr = topic_cubert_model.extract_top_n_words_per_topic(tf_idf, vectorizer, topic_labels,n=30)\n",
    "\n",
    "# topic_cubert_model.topic_sizes_df[\"Name\"] = topic_cubert_model.topic_sizes_df['Topic'].map(name_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c1f81d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../NVIDIA-Devrel/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfddd7",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5aa137",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Running customized BERTopic [without RAPIDS integration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faba3240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 7s, sys: 10.6 s, total: 8min 18s\n",
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Customizing HDBSCAN \n",
    "hdbscan_model_1 = HDBSCAN(min_cluster_size=10, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=5)\n",
    "topic_model_1 = BERTopic(hdbscan_model=hdbscan_model_1)\n",
    "topics_gpu_1, probs_gpu_1 = topic_model_1.fit_transform(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b52db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizng embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81c8bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 5s, sys: 5.58 s, total: 10min 10s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Customizing embedding \n",
    "embeddings = sentence_model.encode(data_0, show_progress_bar=False)\n",
    "topic_model_2 = BERTopic()\n",
    "topics_gpu_2, probs_gpu_2 = topic_model_2.fit_transform(data_0, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f26c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Running customized RAPIDs integrated BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16369b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from cuml.cluster import HDBSCAN as HDBSCAN_gpu\n",
    "from cuml.manifold import UMAP as UMAP_gpu\n",
    "from cuml.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1e183aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 2.98 s, total: 1min 25s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create instances of GPU-accelerated UMAP and HDBSCAN\n",
    "umap_model_3 = UMAP_gpu(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "hdbscan_model_3 = HDBSCAN_gpu(min_samples=10, gen_min_span_tree=True)\n",
    "\n",
    "# Pass the above models to be used in BERTopic\n",
    "topic_model_3 = BERTopic(umap_model=umap_model_3, hdbscan_model=hdbscan_model_3)\n",
    "topics_gpu_3, probs_gpu_3 = topic_model_3.fit_transform(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbf00d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizng embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e948b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 2.55 s, total: 1min 19s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Customizing embedding \n",
    "embeddings = sentence_model.encode(data_0, show_progress_bar=False)\n",
    "embeddings = normalize(embeddings)\n",
    "\n",
    "# Create instances of GPU-accelerated UMAP and HDBSCAN\n",
    "umap_model_4 = UMAP_gpu(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "hdbscan_model_4 = HDBSCAN_gpu(min_samples=10, gen_min_span_tree=True)\n",
    "\n",
    "# Pass the above models to be used in BERTopic\n",
    "topic_model_4 = BERTopic(umap_model=umap_model_4, hdbscan_model=hdbscan_model_4)\n",
    "topics_gpu_4, probs_gpu_4 = topic_model_4.fit_transform(data_0, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
