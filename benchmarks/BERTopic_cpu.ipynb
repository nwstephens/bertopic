{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2975bd6-6d2b-4e55-9709-7907b37f30ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tutorial - Integrating RAPIDS with BERTopic for Topic Modeling \n",
    "(last updated 08-19-2022)\n",
    "\n",
    "In this tutorial, we will use CPU to see the perfromance of BERTopic. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MaartenGr/BERTopic/master/images/logo.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce380d11-ca1f-4a71-b181-05f0eed80988",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installing BERTopic and other necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c9beb-67a4-473c-bee4-55bef923a1c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Installing BERTopic\n",
    "\n",
    "We need to ensure that HDBSCAN package version is compatible with the environment to seamlessly install BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ce7bc6-4382-4683-9051-07d9ad9cab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/rapids\n",
      "\n",
      "  added / updated specs:\n",
      "    - hdbscan\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    certifi-2022.6.15          |   py39hf3d152e_0         155 KB  conda-forge\n",
      "    hdbscan-0.8.28             |   py39hce5d2b2_1         706 KB  conda-forge\n",
      "    openssl-1.1.1q             |       h166bdaf_0         2.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  hdbscan            conda-forge/linux-64::hdbscan-0.8.28-py39hce5d2b2_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  certifi                        2022.5.18.1-py39hf3d152e_0 --> 2022.6.15-py39hf3d152e_0\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "hdbscan-0.8.28       | 706 KB    | ##################################### | 100% \n",
      "certifi-2022.6.15    | 155 KB    | ##################################### | 100% \n",
      "openssl-1.1.1q       | 2.1 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Can be avoided if you want to pip install\n",
    "!conda install -c conda-forge hdbscan -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b814c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::brotlipy==0.7.0=py39h3811e60_1003\n",
      "  - conda-forge/linux-64::certifi==2022.5.18.1=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::cffi==1.15.0=py39h4bc2ebd_0\n",
      "  - conda-forge/noarch::charset-normalizer==2.0.12=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::colorama==0.4.4=pyh9f0ad1d_0\n",
      "  - conda-forge/linux-64::conda==4.12.0=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::conda-package-handling==1.8.0=py39hb9d737c_0\n",
      "  - conda-forge/linux-64::cryptography==36.0.2=py39hd97740a_0\n",
      "  - conda-forge/noarch::idna==3.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pip==22.0.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pycosat==0.6.3=py39h3811e60_1009\n",
      "  - conda-forge/noarch::pycparser==2.21=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyopenssl==22.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pysocks==1.7.1=py39hf3d152e_4\n",
      "  - conda-forge/linux-64::python==3.9.10=h85951f9_2_cpython\n",
      "  - conda-forge/linux-64::python_abi==3.9=2_cp39\n",
      "  - conda-forge/linux-64::readline==8.1=h46c0cb4_0\n",
      "  - conda-forge/noarch::requests==2.27.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::ruamel_yaml==0.15.80=py39h3811e60_1006\n",
      "  - conda-forge/linux-64::setuptools==60.10.0=py39hf3d152e_0\n",
      "  - conda-forge/noarch::six==1.16.0=pyh6c4a22f_0\n",
      "  - conda-forge/linux-64::sqlite==3.37.1=h4ff8645_0\n",
      "  - conda-forge/noarch::tqdm==4.63.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::urllib3==1.26.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::wheel==0.37.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::mamba==0.15.3=py39h951de11_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
      "    libarchive-3.5.2           |       hb890918_3         1.6 MB  conda-forge\n",
      "    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  conda-forge\n",
      "    libgomp-12.1.0             |      h8d9b700_16         459 KB  conda-forge\n",
      "    libssh2-1.10.0             |       haa6b8db_3         234 KB  conda-forge\n",
      "    libxml2-2.9.14             |       h22db469_4         771 KB  conda-forge\n",
      "    libzlib-1.2.12             |       h166bdaf_2          63 KB  conda-forge\n",
      "    ncurses-6.3                |       h27087fc_1        1002 KB  conda-forge\n",
      "    tzdata-2022b               |       h191b570_0         118 KB  conda-forge\n",
      "    xz-5.2.6                   |       h166bdaf_0         409 KB  conda-forge\n",
      "    zlib-1.2.12                |       h166bdaf_2          91 KB  conda-forge\n",
      "    zstd-1.5.2                 |       h8a70e8d_4         448 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.0 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  _openmp_mutex                                   4.5-1_gnu --> 4.5-2_gnu\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  libarchive                               3.5.2-hb890918_2 --> 3.5.2-hb890918_3\n",
      "  libgcc-ng                              11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libgomp                                11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libssh2                                 1.10.0-ha56f1ee_2 --> 1.10.0-haa6b8db_3\n",
      "  libxml2                                 2.9.14-h22db469_0 --> 2.9.14-h22db469_4\n",
      "  libzlib                              1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  ncurses                                    6.3-h9c3ff4c_0 --> 6.3-h27087fc_1\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "  tzdata                                   2022a-h191b570_0 --> 2022b-h191b570_0\n",
      "  xz                                       5.2.5-h516909a_1 --> 5.2.6-h166bdaf_0\n",
      "  zlib                                 1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  zstd                                     1.5.2-h8a70e8d_1 --> 1.5.2-h8a70e8d_4\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \n",
      "libarchive-3.5.2     | 1.6 MB    | ##################################### | 100% \n",
      "ncurses-6.3          | 1002 KB   | ##################################### | 100% \n",
      "tzdata-2022b         | 118 KB    | ##################################### | 100% \n",
      "libxml2-2.9.14       | 771 KB    | ##################################### | 100% \n",
      "libgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \n",
      "zstd-1.5.2           | 448 KB    | ##################################### | 100% \n",
      "libssh2-1.10.0       | 234 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 91 KB     | ##################################### | 100% \n",
      "libzlib-1.2.12       | 63 KB     | ##################################### | 100% \n",
      "xz-5.2.6             | 409 KB    | ##################################### | 100% \n",
      "libgomp-12.1.0       | 459 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ca5cd2-40f0-46d4-9045-655a1eb52a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ubuntu/anaconda3/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909970df-ca58-4ffe-b2d2-3f8e8fea023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in /home/ubuntu/anaconda3/lib/python3.9/site-packages (0.8.27)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.7.3)\n",
      "Requirement already satisfied: cython>=0.27 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (0.29.28)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.0.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94828e08-969b-4a21-8b06-14e742bafd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ubuntu/anaconda3/lib/python3.9/site-packages (22.2.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.9/site-packages (65.0.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/anaconda3/lib/python3.9/site-packages (0.37.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07939659-9a94-49e7-82f4-5efb7b917c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.11.0-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.21.5)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Collecting pyyaml<6.0\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Using cached hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (5.6.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.13.1-cp39-cp39-manylinux1_x86_64.whl (19.1 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.7-py3-none-any.whl\n",
      "Requirement already satisfied: numba>=0.49 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (65.0.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp39-cp39-linux_x86_64.whl size=826749 sha256=7744ad6b2b326f753c67ce6ca3886f5d06a791b05c87abe88662835f64740f40\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/97/2d/1e/d9907e8f806ee949f9effc41004d7f32e862f6f67d9157812d\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, pyyaml, torchvision, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: hdbscan\n",
      "    Found existing installation: hdbscan 0.8.27\n",
      "    Uninstalling hdbscan-0.8.27:\n",
      "      Successfully uninstalled hdbscan-0.8.27\n",
      "Successfully installed bertopic-0.11.0 hdbscan-0.8.28 huggingface-hub-0.8.1 pynndescent-0.5.7 pyyaml-5.4.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.21.1 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feefffe0-1b77-48e7-8d03-1b6d5377ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access folders\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc56072b-c3b7-47c9-add2-16deceab4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access Amazon S3 buckets\n",
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e4352-10fc-4dcb-b6ed-7af6306d6796",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Installing package needed for downloading Wikidata Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26cf1055-8e61-48f0-8486-b8bc21214054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beam\n",
      "  Downloading apache_beam-2.40.0-cp39-cp39-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m200.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0\n",
      "  Downloading orjson-3.7.11-cp39-cp39-manylinux_2_28_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (7.0.0)\n",
      "Collecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.1.0)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m213.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2022.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (4.2.0)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.8.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.27.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Collecting grpcio<2,>=1.33.1\n",
      "  Downloading grpcio-1.47.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m234.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (3.20.1)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from grpcio<2,>=1.33.1->apache_beam) (1.16.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from httplib2<0.21.0,>=0.8->apache_beam) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18848 sha256=fe70632bbfa39dacfd2d0d2e8d28c233001fd0fdf27400323d727c4efdc2128e\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=934c3bb485ae5ae71a73e51ed448a67bbfa14beda01f6a81a11cd05c84c1222b\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=144cf9ca0050bbf41b5ef3770863a9c60f604dba01c78858009c9903abc20265\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built crcmod dill docopt\n",
      "Installing collected packages: docopt, crcmod, pymongo, pydot, proto-plus, orjson, mwparserfromhell, httplib2, grpcio, dill, hdfs, apache_beam\n",
      "Successfully installed apache_beam-2.40.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 grpcio-1.47.0 hdfs-2.7.0 httplib2-0.20.4 mwparserfromhell-0.6.4 orjson-3.7.11 proto-plus-1.22.0 pydot-1.4.2 pymongo-3.12.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba1053f-1605-4109-abb6-d47fb0aac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.3.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m150.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.4.0 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88750f-d68f-468e-a186-1694887fcd28",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Restart the Notebook\n",
    "After installing BERTopic, we should restart the notebook to correctly use the updated packages.\n",
    "\n",
    "Go to Menu -> Runtime → Restart Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fa566-80da-435d-b8e5-81a821ae3d72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2809c2f-66e6-4220-a7a0-b184b2819bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "# import rmm\n",
    "import os\n",
    "# import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('max_colwidth', -1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# rmm.reinitialize(pool_allocator=True,initial_pool_size=5e+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54618d2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Loading\n",
    "\n",
    "We are using 4 publicly available datasets for demonstration\n",
    "- Newsgroups dataset containing roughly 18000 newsgroups posts.\n",
    "- Contract Understanding Atticus Dataset (CUAD) dataset corpus consisting of more than 13,000 labels in 510 commercial legal contracts. \n",
    "- AN4 dataset consisting of spectrogram information that is transformed using Soumith's audio library.\n",
    "- Wikidata corpus built from the Wikipedia dump (https://dumps.wikimedia.org/) contains the contents of multiple Wikipedia articles.\n",
    "\n",
    "To add additional datasets,\n",
    "- Put datasets in data folder. Code is generalized to handle additional datasets that are put in data folder.\n",
    "- But import datasets need to manually added in a list 'bucket_list' that keeps track of all the datasets to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b89449a2-a612-4405-9e8a-691efa1a185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing custom datasets for the lab\n",
    "#All the datasets names are stored in a list 'bucket_list' for ease of tracking \n",
    "bucket_list = []\n",
    "\n",
    "#Importing news dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "bucket_list.append('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0ae535d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code will pickup files from data folder and add it to bucket_list\n",
    "data_folder = \"./data/\"\n",
    "list_files = os.listdir(data_folder)\n",
    "for i in list_files:\n",
    "    if i.find(\".md\")==-1:\n",
    "        bucket_list.append(data_folder+i)\n",
    "\n",
    "bucket_list\n",
    "\n",
    "#Following lines of code help with picking up data files from S3 bucket\n",
    "#Loading data from S3 bucket\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_bucket_name = 'nvidia-rapids'\n",
    "# s3 = boto3.resource('s3')\n",
    "# my_bucket=s3.Bucket(s3_bucket_name)\n",
    "\n",
    "# for file in my_bucket.objects.filter():\n",
    "#     file_name=file.key\n",
    "#     if file_name!=\"data/\" and file_name.find(\"data/\")!=-1:\n",
    "#         bucket_list.append(file.key)\n",
    "# bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd87ab7-5504-45a8-882a-8ce2bb8ba030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793939338a124efe9b3e754737351195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1a4fcc181e44a3818d9b2e46240255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c13f308d344d57b92878c094544251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccb57d371184cbc97b9a99e2abaf721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to /root/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99937ceb42c47f3ac8f1d8be0abf9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz', 'data_wiki']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Wikidata corpus\n",
    "from datasets import load_dataset\n",
    "data_wiki = pd.DataFrame(load_dataset(\"wikipedia\", \"20220301.en\"))\n",
    "bucket_list.append('data_wiki')\n",
    "\n",
    "bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "45ce4a2a-9a49-4518-a4b5-8123d4effc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to read the data files\n",
    "data_wiki_rows = 500000  #As Wikipedia data is huge, to ensure a lower runtime, we can update this variable to pickup these many number of rows \n",
    "for i in range(len(bucket_list)):\n",
    "    if bucket_list[i].find(\".json\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_json(bucket_list[i], orient='columns')\n",
    "    elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "        # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "    elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_pickle(bucket_list[i])\n",
    "    elif bucket_list[i]=='docs':\n",
    "        globals()[\"data_\" + str(i)] = docs\n",
    "    elif bucket_list[i]=='data_wiki':\n",
    "        globals()[\"data_\" + str(i)] = data_wiki.head(data_wiki_rows)\n",
    "    else:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i])\n",
    "\n",
    "#Following lines of code help with reading data files from S3 bucket\n",
    "# for i in range(len(bucket_list)):\n",
    "#     if bucket_list[i].find(\".json\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_json('s3://'+s3_bucket_name+'/'+bucket_list[i], orient='columns')\n",
    "#     elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "#         # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "#     elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_pickle('s3://'+s3_bucket_name+'/'+bucket_list[i])\n",
    "#     elif bucket_list[i]=='docs':\n",
    "#         globals()[\"data_\" + str(i)] = docs\n",
    "#     elif bucket_list[i]=='data_wiki':\n",
    "#         globals()[\"data_\" + str(i)] = data_wiki\n",
    "#     else:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29575b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Sample Text cleaning before fitting into model is applied here. \n",
    "- Most NLP corpus needs customized cleaning based on datasets.\n",
    "- But for the purpose of demonstration, we are only keeps words as topics and removing numbers and other special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "62a59486-feea-4944-8f35-b66417a25529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize dataset 1(in s): 5.07\n",
      "Time to vectorize dataset 2(in s): 5.55\n",
      "Time to vectorize dataset 3(in s): 0.6\n",
      "Time to vectorize dataset 4(in s): 608.67\n"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing step\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "pat1=re.compile(r\"[^a-zA-Z ]+\")\n",
    "pat2=re.compile(r'\\b(?:{})\\b'.format('|'.join(stop)))\n",
    "\n",
    "#Data cleaning for various datasets\n",
    "start_time = time.time()\n",
    "data_0 = pd.DataFrame(data_0)\n",
    "data_0.columns = ['train']\n",
    "data_0 = data_0.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 1(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_1 = data_1.data.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 2(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_2 = data_2[0].astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 3(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_3 = data_3.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 4(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "# def random_function(x):\n",
    "#     return ((str(x).replace(r\"[^a-zA-Z ]+\", \" \")).strip())\n",
    "# data_0 = np.vectorize(random_function)(data_0.data) \n",
    "# data_1 = np.vectorize(random_function)(data_1[0].head(20000))\n",
    "# data_3=data_3.head(10000)\n",
    "# data_0 = data_0.data.apply(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "# data_3=data_3['train'].astype(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e21d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 18846 entries, 0 to 18845\n",
      "Series name: train\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "18846 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 294.5+ KB\n",
      "None\n",
      "Size: 30896294\n",
      "Memory: 301536\n",
      "\n",
      "---------\n",
      "Dataset1 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 510 entries, 0 to 509\n",
      "Series name: data\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "510 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.0+ KB\n",
      "None\n",
      "Size: 34454328\n",
      "Memory: 8160\n",
      "\n",
      "---------\n",
      "Dataset2 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 227143 entries, 0 to 227142\n",
      "Series name: 0\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "227143 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "Size: 15418194\n",
      "Memory: 3634288\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Storing pre-procssed data into a folder to prevent the need of data pre-processing in future iterations\n",
    "processed_data_folder = \"./processed_data/\"\n",
    "for i in range(len(bucket_list)):\n",
    "    print(\"Dataset\"+str(i)+\" info: \")\n",
    "    globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].dropna()\n",
    "    # print(globals()[\"data_\" + str(i)].info())\n",
    "    print(\"Size: \"+str(sys.getsizeof(globals()[\"data_\" + str(i)])))\n",
    "    print(\"Memory: \"+str((globals()[\"data_\" + str(i)]).memory_usage(index=True)))\n",
    "    print()\n",
    "    print(\"---------\")\n",
    "    globals()[\"data_\" + str(i)].to_csv(processed_data_folder+\"data_\"+str(i)+\".csv\", header=[\"data\"],index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5b25f-09e8-4b25-9e9d-cee1cee3c848",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Reloading\n",
    "\n",
    "This step is helpful when data loading and preprocessing has happened once, and we do not want to relaod the data again.\n",
    "- There are some data cleaning steps like removal of text which only consist of numbers or duplicate text, but it should be customized based on dataset\n",
    "- You can avoid this step during the first run as all the dataset variables point to right dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95d6f887-400e-47fa-b286-b0732ef8ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from the processed folder so that we can skip data load and preprocessing in future runs\n",
    "read_from_processed = False   #Change it to True if you want to load data from processed folder\n",
    "bucket_list = ['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz', 'data_wiki']\n",
    "if read_from_processed==True:\n",
    "    for i in range(len(bucket_list)):\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(\"processed_data/data_\"+str(i)+\".csv\", dtype=str,index_col=None)\n",
    "        # We can do some additional cleaning like sentences that only have numbers\n",
    "        globals()[\"data_\" + str(i)].data = globals()[\"data_\" + str(i)].apply(lambda r: r['data'] if type(r['data'])!=float else np.nan, axis=1)\n",
    "        globals()[\"data_\" + str(i)].dropna(inplace=True)\n",
    "        globals()[\"data_\" + str(i)].drop_duplicates(inplace=True)\n",
    "        globals()[\"data_\" + str(i)].reset_index(drop=True, inplace=True)\n",
    "        globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f32a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Topic Modeling using BERTopic [without RAPIDS integration]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2d75e-8feb-478a-9e5d-5ef92df3c97f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training the BERTopic model\n",
    "\n",
    "For instantiating BERTopic, you need to run the following commands\n",
    "- topic_model = BERTopic(verbose=True)\n",
    "- topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "We can customize BERTopic by using the different parameters mentioned in [BERTopic](https://maartengr.github.io/BERTopic/faq.html). You can add parameters like hdbscan_model=HDBSCAN(), umap_model=UMAP(), language=\"english\" or calculate_probabilities=True in BERTopic() to customize the model.\n",
    "\n",
    "We will also calculate the topic probabilities. However, this can slow down BERTopic significantly at large amounts of data (>100_000 documents). It is advised to turn this off if you want to speed up the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78eda8ab-780c-4674-b935-608a89ea84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 06:58:34 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfafff5af8d4bcc9d1a423d41c51614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e10fcae7ba34d2b964a8620fc87035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec16a638f5a405685208bb3aa6ee7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace79ee2089947748389ae396d3010ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f07ff884f834f5e8b139ef9f2e4ca34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94bfe01ba7346e6a148c2feee8bb474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea92b0239f764135b70d8ccc25b8e5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b9d9669d3c46c78900f51409f59315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c08707259154d6bac931370d149c567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a33e75bdaf4e11ae0be4a883f0d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871e585f2c1e4dc2a014c53c0e216811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26269336224940d6916af890c55cddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e288bf297f0942d496ce79e4540bf053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e124a63105c94ab1b707c5a238dfed86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aafe58752f544778fad109ad4aa6733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:01:16,851 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:01:37,088 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:01:40,404 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:01:58,424 - BERTopic - Reduced number of topics from 295 to 236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 204.01\n",
      "Mon Aug 15 07:04:54 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a514a032be4f6090bd44509b4d99e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:05:05,580 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:05:07,816 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:05:07,834 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:05:15,570 - BERTopic - Reduced number of topics from 14 to 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset2(in s): 20.96\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bucket_list)):\n",
    "    if i>1:   #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"topic_model_cpu_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",)\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    viz_topics = globals()[\"topic_model_cpu_\" + str(i)].visualize_topics()\n",
    "    # print(viz_topics.show())\n",
    "    viz_topics.write_html(\"results/viz_topics_cpu_\"+str(i)+\".html\")\n",
    "    viz_barchart = globals()[\"topic_model_cpu_\" + str(i)].visualize_barchart()\n",
    "    # print(viz_barchart.show())\n",
    "    viz_barchart.write_html(\"results/viz_barchart_cpu_\"+str(i)+\".html\")\n",
    "    viz_docs = globals()[\"topic_model_cpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "    viz_docs.write_html(\"results/viz_docs_cpu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "578df4a8-754e-4501-bd66-46fccc22e5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 07:06:11 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aca9c55202486fad3745d4dded6763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:10:21,969 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:49:31,586 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:49:56,555 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:50:11,669 - BERTopic - Reduced number of topics from 1147 to 473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset3(in s): 2639.73\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bucket_list)):\n",
    "    if i!=2:   #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"topic_model_cpu_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",)\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    # viz_topics = globals()[\"topic_model_cpu_\" + str(i)].visualize_topics()\n",
    "    # # print(viz_topics.show())\n",
    "    # viz_topics.write_html(\"results/viz_topics_cpu_\"+str(i)+\".html\")\n",
    "    # viz_barchart = globals()[\"topic_model_cpu_\" + str(i)].visualize_barchart()\n",
    "    # # print(viz_barchart.show())\n",
    "    # viz_barchart.write_html(\"results/viz_barchart_cpu_\"+str(i)+\".html\")\n",
    "    # viz_docs = globals()[\"topic_model_cpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "    # viz_docs.write_html(\"results/viz_docs_cpu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7541c-23e0-417f-8f8c-7830c5e9dc1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting topics\n",
    "\n",
    "- After fitting the model, we usually look at the most frequent topics first as they best represent the collection of documents.\n",
    "- -1 should be ignored as it refers to outliers. \n",
    "\n",
    "**NOTE**: Due to stochastic nature of UMAP stage of BERTopic, the topics might differ across runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121e31e1-97a7-464b-8e6d-259c82c43d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7084</td>\n",
       "      <td>-1_edu_com_the_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>0_game_team_games_baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>771</td>\n",
       "      <td>1_key_clipper_encryption_chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>353</td>\n",
       "      <td>2_god_jesus_christ_bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>339</td>\n",
       "      <td>3_israel_israeli_arab_jews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>230</td>\n",
       "      <td>11</td>\n",
       "      <td>230_rh_liar_lunatic_bissell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>231</td>\n",
       "      <td>10</td>\n",
       "      <td>231_land_property_wage_kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>232_xvertext_sussex_mppa_syma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>233</td>\n",
       "      <td>10</td>\n",
       "      <td>233_convex_visser_pubic_hairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>234</td>\n",
       "      <td>10</td>\n",
       "      <td>234_cd_apple_cdrom_applelink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                           Name\n",
       "0   -1      7084   -1_edu_com_the_subject       \n",
       "1    0      1270   0_game_team_games_baseball   \n",
       "2    1      771    1_key_clipper_encryption_chip\n",
       "3    2      353    2_god_jesus_christ_bible     \n",
       "4    3      339    3_israel_israeli_arab_jews   \n",
       "..  ..      ...                           ...   \n",
       "231  230    11     230_rh_liar_lunatic_bissell  \n",
       "232  231    10     231_land_property_wage_kids  \n",
       "233  232    10     232_xvertext_sussex_mppa_syma\n",
       "234  233    10     233_convex_visser_pubic_hairs\n",
       "235  234    10     234_cd_apple_cdrom_applelink \n",
       "\n",
       "[236 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d41589-6d68-4784-952c-8be225658fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 0.012488819768867251),\n",
       " ('team', 0.010677236700507593),\n",
       " ('games', 0.009776389054572182),\n",
       " ('baseball', 0.008980899273839325),\n",
       " ('year', 0.00831041850235353),\n",
       " ('season', 0.007813869938817934),\n",
       " ('players', 0.007424436509869103),\n",
       " ('league', 0.006786845223771858),\n",
       " ('hockey', 0.006397511235252266),\n",
       " ('hit', 0.0058796397520957085)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e962670c-0ec3-4435-b3c6-7b6c4a2363b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>142</td>\n",
       "      <td>-1_agreement_party_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0_party_agreement_shall_company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1_distributor_agreement_party_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2_shall_party_agreement_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3_sponsorship_sponsor_agreement_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4_maintenance_agreement_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5_endorsement_contract_agreement_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>6_co_verticalnet_branding_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>7_hosting_customer_software_agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_msl_ibm_customer_outsourcing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9_spinco_group_intellectual_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10_transportation_shipper_transporter_shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_franchise_franchisee_us_pretzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12_reseller_agreement_ehave_touchstar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name\n",
       "0  -1      142    -1_agreement_party_shall_contract          \n",
       "1   0      67     0_party_agreement_shall_company            \n",
       "2   1      61     1_distributor_agreement_party_ex           \n",
       "3   2      38     2_shall_party_agreement_product            \n",
       "4   3      32     3_sponsorship_sponsor_agreement_contract   \n",
       "5   4      31     4_maintenance_agreement_shall_contract     \n",
       "6   5      22     5_endorsement_contract_agreement_ex        \n",
       "7   6      21     6_co_verticalnet_branding_party            \n",
       "8   7      20     7_hosting_customer_software_agreement      \n",
       "9   8      17     8_msl_ibm_customer_outsourcing             \n",
       "10  9      16     9_spinco_group_intellectual_property       \n",
       "11  10     15     10_transportation_shipper_transporter_shall\n",
       "12  11     15     11_franchise_franchisee_us_pretzel         \n",
       "13  12     13     12_reseller_agreement_ehave_touchstar      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d8ef95-dee9-4086-9acb-11b479700b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('party', 0.03041899480135238),\n",
       " ('agreement', 0.029226079792114903),\n",
       " ('shall', 0.024473584735500518),\n",
       " ('company', 0.02307833617389313),\n",
       " ('contract', 0.02116111833557621),\n",
       " ('agent', 0.015861823085333314),\n",
       " ('parties', 0.015733141752975032),\n",
       " ('related', 0.015681218009116288),\n",
       " ('the', 0.014880079515670825),\n",
       " ('details', 0.01455191449561325)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f89a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a18158-95d4-47f1-b9e5-f8e07c20169c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall time is calculated for only the topic modeling piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame()\n",
    "results['dataset'] = ['news_dataset', 'CUAD_dataset','Amazon_dataset']\n",
    "results['BERTopic_time'] = ['end_time_cpu_0', 'end_time_cpu_1','end_time_cpu_2']\n",
    "results.to_csv('./results/overall_results_cpu.csv', index = False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febe4bc",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23aff4-7ed1-4f16-989e-72522b4989cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Running customized BERTopic [without RAPIDS integration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73733112-153b-435b-85bd-cf230e52417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Customizing HDBSCAN \n",
    "hdbscan_model_1 = HDBSCAN(min_cluster_size=10, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=5)\n",
    "topic_model_1 = BERTopic(hdbscan_model=hdbscan_model_1)\n",
    "topics_cpu_1, probs_cpu_1 = topic_model_1.fit_transform(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73908622-b88d-4df2-80ca-0db23700d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizng embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f6e6e-c70f-45d3-9a45-6d9b34fcb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Customizing embedding \n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "topic_model_2 = BERTopic()\n",
    "topics_cpu_2, probs_cpu_2 = topic_model_2.fit_transform(data_0, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
