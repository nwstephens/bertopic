{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c08df7",
   "metadata": {},
   "source": [
    "# Showcase BERTopic and RAPIDs Integrated cuBERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce380d11-ca1f-4a71-b181-05f0eed80988",
   "metadata": {},
   "source": [
    "# Installing BERTopic and other necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9badf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732ecb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (1837.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp39-cp39-linux_x86_64.whl (23.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp39-cp39-linux_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (9.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ce7bc6-4382-4683-9051-07d9ad9cab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/rapids\n",
      "\n",
      "  added / updated specs:\n",
      "    - hdbscan\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    certifi-2022.6.15          |   py39hf3d152e_0         155 KB  conda-forge\n",
      "    hdbscan-0.8.28             |   py39hce5d2b2_1         706 KB  conda-forge\n",
      "    openssl-1.1.1q             |       h166bdaf_0         2.1 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  hdbscan            conda-forge/linux-64::hdbscan-0.8.28-py39hce5d2b2_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  certifi                        2022.5.18.1-py39hf3d152e_0 --> 2022.6.15-py39hf3d152e_0\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "hdbscan-0.8.28       | 706 KB    | ##################################### | 100% \n",
      "certifi-2022.6.15    | 155 KB    | ##################################### | 100% \n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "openssl-1.1.1q       | 2.1 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge hdbscan -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b814c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::brotlipy==0.7.0=py39h3811e60_1003\n",
      "  - conda-forge/linux-64::certifi==2022.5.18.1=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::cffi==1.15.0=py39h4bc2ebd_0\n",
      "  - conda-forge/noarch::charset-normalizer==2.0.12=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::colorama==0.4.4=pyh9f0ad1d_0\n",
      "  - conda-forge/linux-64::conda==4.12.0=py39hf3d152e_0\n",
      "  - conda-forge/linux-64::conda-package-handling==1.8.0=py39hb9d737c_0\n",
      "  - conda-forge/linux-64::cryptography==36.0.2=py39hd97740a_0\n",
      "  - conda-forge/noarch::idna==3.3=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pip==22.0.4=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pycosat==0.6.3=py39h3811e60_1009\n",
      "  - conda-forge/noarch::pycparser==2.21=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyopenssl==22.0.0=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::pysocks==1.7.1=py39hf3d152e_4\n",
      "  - conda-forge/linux-64::python==3.9.10=h85951f9_2_cpython\n",
      "  - conda-forge/linux-64::python_abi==3.9=2_cp39\n",
      "  - conda-forge/linux-64::readline==8.1=h46c0cb4_0\n",
      "  - conda-forge/noarch::requests==2.27.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::ruamel_yaml==0.15.80=py39h3811e60_1006\n",
      "  - conda-forge/linux-64::setuptools==60.10.0=py39hf3d152e_0\n",
      "  - conda-forge/noarch::six==1.16.0=pyh6c4a22f_0\n",
      "  - conda-forge/linux-64::sqlite==3.37.1=h4ff8645_0\n",
      "  - conda-forge/noarch::tqdm==4.63.0=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::urllib3==1.26.9=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::wheel==0.37.1=pyhd8ed1ab_0\n",
      "  - conda-forge/linux-64::mamba==0.15.3=py39h951de11_0\n",
      "done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
      "    libarchive-3.5.2           |       hb890918_3         1.6 MB  conda-forge\n",
      "    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  conda-forge\n",
      "    libgomp-12.1.0             |      h8d9b700_16         459 KB  conda-forge\n",
      "    libxml2-2.9.14             |       h22db469_3         781 KB  conda-forge\n",
      "    libzlib-1.2.12             |       h166bdaf_2          63 KB  conda-forge\n",
      "    ncurses-6.3                |       h27087fc_1        1002 KB  conda-forge\n",
      "    tzdata-2022b               |       h191b570_0         118 KB  conda-forge\n",
      "    zlib-1.2.12                |       h166bdaf_2          91 KB  conda-forge\n",
      "    zstd-1.5.2                 |       h8a70e8d_3         450 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.4 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  _openmp_mutex                                   4.5-1_gnu --> 4.5-2_gnu\n",
      "  ca-certificates                    2022.5.18.1-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  libarchive                               3.5.2-hb890918_2 --> 3.5.2-hb890918_3\n",
      "  libgcc-ng                              11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libgomp                                11.2.0-h1d223b6_14 --> 12.1.0-h8d9b700_16\n",
      "  libxml2                                 2.9.14-h22db469_0 --> 2.9.14-h22db469_3\n",
      "  libzlib                              1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  ncurses                                    6.3-h9c3ff4c_0 --> 6.3-h27087fc_1\n",
      "  openssl                                 1.1.1o-h166bdaf_0 --> 1.1.1q-h166bdaf_0\n",
      "  tzdata                                   2022a-h191b570_0 --> 2022b-h191b570_0\n",
      "  zlib                                 1.2.11-h36c2ea0_1013 --> 1.2.12-h166bdaf_2\n",
      "  zstd                                     1.5.2-h8a70e8d_1 --> 1.5.2-h8a70e8d_3\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \n",
      "zstd-1.5.2           | 450 KB    | ##################################### | 100% \n",
      "libxml2-2.9.14       | 781 KB    | ##################################### | 100% \n",
      "libzlib-1.2.12       | 63 KB     | ##################################### | 100% \n",
      "libgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \n",
      "libgomp-12.1.0       | 459 KB    | ##################################### | 100% \n",
      "ncurses-6.3          | 1002 KB   | ##################################### | 100% \n",
      "libarchive-3.5.2     | 1.6 MB    | ##################################### | 100% \n",
      "tzdata-2022b         | 118 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 91 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472bbb0c-b542-41ab-adee-d94f29c0f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/envs/rapids/lib/python3.9/site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hdbscan in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.8.28)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/rapids/lib/python3.9/site-packages (0.37.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (0.24.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (0.29.30)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip hdbscan wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6cbc8d-55b0-4c25-8cd2-ee7411fdd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.11.0-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: umap-learn>=0.5.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.24.2)\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: hdbscan>=0.8.28 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (0.8.28)\n",
      "Collecting pyyaml<6.0\n",
      "  Downloading PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 kB\u001b[0m \u001b[31m148.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cython>=0.27 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.30)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.6.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.12.1+cu113)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/rapids/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.1+cu113)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.49 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.7)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (60.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (765 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.1/765.1 kB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m170.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125940 sha256=ca7b23b61f6c10656ca0f7f0442d427b64aa0a03f27507b792b8d8ec7dff7282\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, tenacity, regex, pyyaml, filelock, plotly, nltk, huggingface-hub, transformers, sentence-transformers, bertopic\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.6.0 requires cupy-cuda115, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bertopic-0.11.0 filelock-3.8.0 huggingface-hub-0.8.1 nltk-3.7 plotly-5.10.0 pyyaml-5.4.1 regex-2022.7.25 sentence-transformers-2.2.2 sentencepiece-0.1.97 tenacity-8.0.1 tokenizers-0.12.1 transformers-4.21.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bertopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feefffe0-1b77-48e7-8d03-1b6d5377ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access folders\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc56072b-c3b7-47c9-add2-16deceab4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access Amazon S3 buckets\n",
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26cf1055-8e61-48f0-8486-b8bc21214054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beam\n",
      "  Downloading apache_beam-2.40.0-cp39-cp39-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0\n",
      "  Downloading orjson-3.7.11-cp39-cp39-manylinux_2_28_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (4.2.0)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.5.0)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.1.0)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (1.21.6)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.8.2)\n",
      "Collecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2,>=1.33.1\n",
      "  Downloading grpcio-1.47.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m161.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2.27.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (2022.1)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from apache_beam) (7.0.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from grpcio<2,>=1.33.1->apache_beam) (1.16.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from httplib2<0.21.0,>=0.8->apache_beam) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2022.6.15)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18848 sha256=bfe1c5d687dc6b1ac38ced6a0eb0ba6eb1174e85b10a5999609a4134ac8750a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=027e2b07bd915b049d15ebf0f477fe46954ba0966a06034746f2a808c6e83865\n",
      "  Stored in directory: /root/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=cc1d2cf70592d4184dd229398a81f25f8c4b55e57ce030519950396ee8c79915\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built crcmod dill docopt\n",
      "Installing collected packages: docopt, crcmod, pymongo, pydot, proto-plus, orjson, mwparserfromhell, httplib2, grpcio, dill, hdfs, apache_beam\n",
      "Successfully installed apache_beam-2.40.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 grpcio-1.47.0 hdfs-2.7.0 httplib2-0.20.4 mwparserfromhell-0.6.4 orjson-3.7.11 proto-plus-1.22.0 pydot-1.4.2 pymongo-3.12.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba1053f-1605-4109-abb6-d47fb0aac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.3.1.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from datasets) (0.8.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.4.0 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fa566-80da-435d-b8e5-81a821ae3d72",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2809c2f-66e6-4220-a7a0-b184b2819bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import rmm\n",
    "import os\n",
    "# import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('max_colwidth', -1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# rmm.reinitialize(pool_allocator=True,initial_pool_size=5e+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54618d2e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b89449a2-a612-4405-9e8a-691efa1a185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing custom datasets for the lab\n",
    "#All the datasets names are stored in a list 'bucket_list' for ease of tracking \n",
    "bucket_list = []\n",
    "\n",
    "#Importing news dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "bucket_list.append('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae535d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code will pickup files from data folder and add it to bucket_list\n",
    "data_folder = \"./data/\"\n",
    "list_files = os.listdir(data_folder)\n",
    "for i in list_files:\n",
    "    if i.find(\".md\")==-1:\n",
    "        bucket_list.append(data_folder+i)\n",
    "bucket_list\n",
    "\n",
    "#Following lines of code help with picking up data files from S3 bucket\n",
    "#Loading data from S3 bucket\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_bucket_name = 'nvidia-rapids'\n",
    "# s3 = boto3.resource('s3')\n",
    "# my_bucket=s3.Bucket(s3_bucket_name)\n",
    "\n",
    "# for file in my_bucket.objects.filter():\n",
    "#     file_name=file.key\n",
    "#     if file_name!=\"data/\" and file_name.find(\"data/\")!=-1:\n",
    "#         bucket_list.append(file.key)\n",
    "# bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd87ab7-5504-45a8-882a-8ce2bb8ba030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Wikidata corpus\n",
    "from datasets import load_dataset\n",
    "data_wiki = pd.DataFrame(load_dataset(\"wikipedia\", \"20220301.en\"))\n",
    "bucket_list.append('data_wiki')\n",
    "\n",
    "bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ce4a2a-9a49-4518-a4b5-8123d4effc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to read the data files\n",
    "data_wiki_rows = 1000000  #As Wikipedia data is huge, to ensure a lower runtime, we can update this variable to pickup these many number of rows \n",
    "for i in range(len(bucket_list)):\n",
    "    if bucket_list[i].find(\".json\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_json(bucket_list[i], orient='columns')\n",
    "    elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "        # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "    elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_pickle(bucket_list[i])\n",
    "    elif bucket_list[i]=='docs':\n",
    "        globals()[\"data_\" + str(i)] = docs\n",
    "    elif bucket_list[i]=='data_wiki':\n",
    "        globals()[\"data_\" + str(i)] = data_wiki.head(data_wiki_rows)\n",
    "    else:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i])\n",
    "\n",
    "#Following lines of code help with reading data files from S3 bucket\n",
    "# for i in range(len(bucket_list)):\n",
    "#     if bucket_list[i].find(\".json\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_json('s3://'+s3_bucket_name+'/'+bucket_list[i], orient='columns')\n",
    "#     elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "#         # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "#     elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_pickle('s3://'+s3_bucket_name+'/'+bucket_list[i])\n",
    "#     elif bucket_list[i]=='docs':\n",
    "#         globals()[\"data_\" + str(i)] = docs\n",
    "#     elif bucket_list[i]=='data_wiki':\n",
    "#         globals()[\"data_\" + str(i)] = data_wiki\n",
    "#     else:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29575b",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62a59486-feea-4944-8f35-b66417a25529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize dataset 1(in s): 5.08\n",
      "Time to vectorize dataset 2(in s): 5.49\n",
      "Time to vectorize dataset 3(in s): 0.58\n"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing step\n",
    "#Sample Text cleaning before fitting into model\n",
    "#Most NLP corpus needs customized cleaning based on datasets\n",
    "#But for the purpose of demonstration, we are only keeps words as topics and removing numbers and other special characters\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "pat1=re.compile(r\"[^a-zA-Z ]+\")\n",
    "pat2=re.compile(r'\\b(?:{})\\b'.format('|'.join(stop)))\n",
    "\n",
    "#Data cleaning for various datasets\n",
    "start_time = time.time()\n",
    "data_0 = pd.DataFrame(data_0)\n",
    "data_0.columns = ['train']\n",
    "data_0 = data_0.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 1(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_1 = data_1.data.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 2(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_2 = data_2[0].astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 3(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_3 = data_3.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 4(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "# def random_function(x):\n",
    "#     return ((str(x).replace(r\"[^a-zA-Z ]+\", \" \")).strip())\n",
    "# data_0 = np.vectorize(random_function)(data_0.data) \n",
    "# data_1 = np.vectorize(random_function)(data_1[0].head(20000))\n",
    "# data_3=data_3.head(10000)\n",
    "# data_0 = data_0.data.apply(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "# data_3=data_3['train'].astype(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e21d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 18846 entries, 0 to 18845\n",
      "Series name: train\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "18846 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 294.5+ KB\n",
      "None\n",
      "Size: 30896294\n",
      "Memory: 301536\n",
      "\n",
      "---------\n",
      "Dataset1 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 510 entries, 0 to 509\n",
      "Series name: data\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "510 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.0+ KB\n",
      "None\n",
      "Size: 34454328\n",
      "Memory: 8160\n",
      "\n",
      "---------\n",
      "Dataset2 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 227143 entries, 0 to 227142\n",
      "Series name: 0\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "227143 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "Size: 15418194\n",
      "Memory: 3634288\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Storing pre-procssed data into a folder to prevent the need of data pre-processing in future iterations\n",
    "processed_data_folder = \"./processed_data/\"\n",
    "for i in range(len(bucket_list)):\n",
    "    print(\"Dataset\"+str(i)+\" info: \")\n",
    "    globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].dropna()\n",
    "    print(globals()[\"data_\" + str(i)].info())\n",
    "    print(\"Size: \"+str(sys.getsizeof(globals()[\"data_\" + str(i)])))\n",
    "    print(\"Memory: \"+str((globals()[\"data_\" + str(i)]).memory_usage(index=True)))\n",
    "    print()\n",
    "    print(\"---------\")\n",
    "    globals()[\"data_\" + str(i)].to_csv(processed_data_folder+\"data_\"+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f32a1",
   "metadata": {},
   "source": [
    "# Topic Modeling using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786d4910-cc16-44e1-8847-95ef19f4f518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fcc62b47d04e46bfbea7db4fc796fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16d3bfab3a14ffdb8f158a6f502051f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c961c7a5b91749c2892d5ec5ead50469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0efcc4094d44f64a015f15587112707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d760e03562f4d62a4f7c5229435531d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f351b10a3f4e65804fd7d2b6464218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017e2703605e49fcae574a65db37a26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe79212270454dbf973c9513d62a49ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27576f5d2e66471e971a1d5df657c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09240f3e87df4d63bab0c62b751c958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f69fc9da631469c87898b346b94bf29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3373e5841bf4929a0ec17840dc2e697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccf642ddd5b4ab49e16be851cc497d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284f5879bd6a4a5ebfc057df3db13330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 73.04\n",
      "Time to perform topic modeling for dataset2(in s): 10.13\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bucket_list)):\n",
    "    # if i>1:   #Counter to run specific datasets\n",
    "    #     continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    globals()[\"topic_model_gpu_\" + str(i)] = BERTopic()\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_gpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    viz_topics = globals()[\"topic_model_cpu_\" + str(i)].visualize_topics()\n",
    "    # print(viz_topics.show())\n",
    "    viz_topics.write_html(\"viz_topics_\"+str(i)+\".html\")\n",
    "    viz_barchart = globals()[\"topic_model_cpu_\" + str(i)].visualize_barchart()\n",
    "    # print(viz_barchart.show())\n",
    "    viz_barchart.write_html(\"viz_barchart_\"+str(i)+\".html\")\n",
    "    viz_docs = globals()[\"topic_model_cpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "    viz_docs.write_html(\"viz_docs_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3517e44f-442a-4443-bad6-c0423ad18b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7075</td>\n",
       "      <td>-1_edu_com_the_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>857</td>\n",
       "      <td>0_baseball_game_year_team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>1_key_encryption_clipper_chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>306</td>\n",
       "      <td>2_gun_guns_firearms_militia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>279</td>\n",
       "      <td>3_car_cars_mustang_engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>291</td>\n",
       "      <td>10</td>\n",
       "      <td>291_kent_mcs_mhamilto_nimitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>292</td>\n",
       "      <td>10</td>\n",
       "      <td>292_motivated_religously_religiously_frank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>293</td>\n",
       "      <td>10</td>\n",
       "      <td>293_liar_rh_lunatic_gathered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>294</td>\n",
       "      <td>10</td>\n",
       "      <td>294_clinton_reno_federal_batf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>295</td>\n",
       "      <td>10</td>\n",
       "      <td>295_viewer_tmc_dominik_dundee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                        Name\n",
       "0   -1      7075   -1_edu_com_the_subject                    \n",
       "1    0      857    0_baseball_game_year_team                 \n",
       "2    1      780    1_key_encryption_clipper_chip             \n",
       "3    2      306    2_gun_guns_firearms_militia               \n",
       "4    3      279    3_car_cars_mustang_engine                 \n",
       "..  ..      ...                          ...                 \n",
       "292  291    10     291_kent_mcs_mhamilto_nimitz              \n",
       "293  292    10     292_motivated_religously_religiously_frank\n",
       "294  293    10     293_liar_rh_lunatic_gathered              \n",
       "295  294    10     294_clinton_reno_federal_batf             \n",
       "296  295    10     295_viewer_tmc_dominik_dundee             \n",
       "\n",
       "[297 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69e9a24-fea9-4aa8-8f39-d1c5c2f98fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseball', 0.010475889952278009),\n",
       " ('game', 0.00888648349492628),\n",
       " ('year', 0.00861105105443462),\n",
       " ('team', 0.008469061639788836),\n",
       " ('players', 0.007548359382496322),\n",
       " ('hit', 0.007530132235814117),\n",
       " ('braves', 0.007529716506300251),\n",
       " ('games', 0.007309955232745601),\n",
       " ('runs', 0.0068805481485816585),\n",
       " ('pitching', 0.006444728759187212)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87c188c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>159</td>\n",
       "      <td>-1_agreement_party_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0_distributor_agreement_party_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1_agreement_party_company_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2_sponsorship_sponsor_agreement_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>3_maintenance_agreement_shall_vendor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>4_endorsement_contract_agreement_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5_co_verticalnet_branding_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>6_hosting_customer_software_agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>7_product_shall_agreement_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_msl_ibm_customer_outsourcing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9_spinco_group_intellectual_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>10_franchise_franchisee_us_pretzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_transportation_shipper_transporter_shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12_reseller_agreement_ehave_touchstar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13_shall_party_astellas_roundup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name\n",
       "0  -1      159    -1_agreement_party_shall_contract          \n",
       "1   0      59     0_distributor_agreement_party_ex           \n",
       "2   1      58     1_agreement_party_company_contract         \n",
       "3   2      32     2_sponsorship_sponsor_agreement_contract   \n",
       "4   3      31     3_maintenance_agreement_shall_vendor       \n",
       "5   4      22     4_endorsement_contract_agreement_ex        \n",
       "6   5      21     5_co_verticalnet_branding_party            \n",
       "7   6      20     6_hosting_customer_software_agreement      \n",
       "8   7      19     7_product_shall_agreement_party            \n",
       "9   8      17     8_msl_ibm_customer_outsourcing             \n",
       "10  9      16     9_spinco_group_intellectual_property       \n",
       "11  10     16     10_franchise_franchisee_us_pretzel         \n",
       "12  11     15     11_transportation_shipper_transporter_shall\n",
       "13  12     13     12_reseller_agreement_ehave_touchstar      \n",
       "14  13     12     13_shall_party_astellas_roundup            "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "156b27b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distributor', 0.037079294786719036),\n",
       " ('agreement', 0.03270249443242861),\n",
       " ('party', 0.030775109377613134),\n",
       " ('ex', 0.026850783858802674),\n",
       " ('contract', 0.026406133174178816),\n",
       " ('shall', 0.026399277154165558),\n",
       " ('license', 0.022018705365346277),\n",
       " ('related', 0.019295638152168407),\n",
       " ('licensee', 0.01887526968572377),\n",
       " ('parts', 0.01856044081063633)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f89a0",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame()\n",
    "results['dataset'] = ['news_dataset', 'CUAD_dataset','Amazon_dataset']\n",
    "results['BERTopic_time'] = ['end_time_cpu_0', 'end_time_cpu_1','end_time_cpu_2']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febe4bc",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23aff4-7ed1-4f16-989e-72522b4989cb",
   "metadata": {},
   "source": [
    "## Running customized BERTopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee27bc6-3036-48a9-abfd-394f5842e2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 5.78 s, total: 2min 8s\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Customizing HDBSCAN \n",
    "hdbscan_model_1 = HDBSCAN(min_cluster_size=10, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=5)\n",
    "topic_model_1 = BERTopic(hdbscan_model=hdbscan_model_1)\n",
    "topics_cpu_1, probs_cpu_1 = topic_model_1.fit_transform(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b95030-7ed1-4943-89b4-f584fbc6aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizng embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c484245e-3fcd-41b0-80b1-e1a18e1f1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 5.59 s, total: 2min 6s\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Customizing embedding \n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "topic_model_2 = BERTopic()\n",
    "topics_cpu_2, probs_cpu_2 = topic_model_2.fit_transform(data_0, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
